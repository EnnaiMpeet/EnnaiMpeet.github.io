---
layout: post
title: xmm | 肖美美
categories: [blog]
description: xmmblog
keywords:  
mermaid: false
sequence: false
flow: false
mathjax: false
mindmap: false
mindmap2: false
topmost: true
---

<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLaDA研究解析：突破自回归范式的新探索</title>
    <style>
        :root {
            --primary: #2c3e50;
            --secondary: #3498db;
            --accent: #e74c3c;
            --light: #ecf0f1;
            --dark: #34495e;
            --text: #2c3e50;
            --border: #bdc3c7;
        }
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }
        
        body {
            background-color: #f9f9f9;
            color: var(--text);
            line-height: 1.6;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        
        header {
            background: linear-gradient(135deg, var(--primary), var(--dark));
            color: white;
            padding: 60px 20px;
            text-align: center;
            border-radius: 0 0 10px 10px;
            margin-bottom: 40px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
        }
        
        h1 {
            font-size: 2.8rem;
            margin-bottom: 15px;
            font-weight: 700;
        }
        
        .subtitle {
            font-size: 1.4rem;
            opacity: 0.9;
            max-width: 800px;
            margin: 0 auto;
        }
        
        section {
            background: white;
            border-radius: 10px;
            padding: 30px;
            margin-bottom: 30px;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.05);
            border-left: 5px solid var(--secondary);
        }
        
        h2 {
            color: var(--primary);
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--light);
            font-size: 1.8rem;
        }
        
        h3 {
            color: var(--dark);
            margin: 25px 0 15px;
            font-size: 1.4rem;
        }
        
        p {
            margin-bottom: 15px;
            font-size: 1.1rem;
        }
        
        .highlight {
            background-color: #f8f9fa;
            border-left: 4px solid var(--secondary);
            padding: 15px 20px;
            margin: 20px 0;
            border-radius: 0 5px 5px 0;
        }
        
        .formula {
            background-color: #f5f7f9;
            padding: 15px;
            border-radius: 5px;
            margin: 20px 0;
            text-align: center;
            font-family: 'Cambria Math', serif;
            font-size: 1.2rem;
            overflow-x: auto;
        }
        
        .method-steps {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
            margin: 25px 0;
        }
        
        .step {
            flex: 1;
            min-width: 250px;
            background: var(--light);
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 3px 6px rgba(0, 0, 0, 0.08);
        }
        
        .step-number {
            display: inline-block;
            background: var(--secondary);
            color: white;
            width: 30px;
            height: 30px;
            border-radius: 50%;
            text-align: center;
            line-height: 30px;
            margin-right: 10px;
            font-weight: bold;
        }
        
        .results-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
            gap: 20px;
            margin: 25px 0;
        }
        
        .result-card {
            background: white;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 3px 10px rgba(0, 0, 0, 0.08);
            border-top: 4px solid var(--secondary);
        }
        
        .result-title {
            font-weight: bold;
            margin-bottom: 10px;
            color: var(--dark);
        }
        
        .comparison {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
            margin: 25px 0;
        }
        
        .comparison-item {
            flex: 1;
            min-width: 300px;
            padding: 20px;
            border-radius: 8px;
            background: var(--light);
        }
        
        .comparison-title {
            font-weight: bold;
            text-align: center;
            margin-bottom: 15px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--border);
        }
        
        .limitation {
            background-color: #fff8f8;
            border-left: 4px solid var(--accent);
            padding: 15px 20px;
            margin: 20px 0;
            border-radius: 0 5px 5px 0;
        }
        
        .conclusion {
            background: linear-gradient(135deg, #f5f7fa, #e4e8ee);
            padding: 25px;
            border-radius: 10px;
            margin: 30px 0;
            text-align: center;
        }
        
        footer {
            text-align: center;
            padding: 30px 0;
            margin-top: 40px;
            color: var(--dark);
            border-top: 1px solid var(--border);
        }
        
        @media (max-width: 768px) {
            h1 {
                font-size: 2.2rem;
            }
            
            .subtitle {
                font-size: 1.2rem;
            }
            
            section {
                padding: 20px;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>LLaDA研究解析</h1>
            <p class="subtitle">突破自回归范式的新探索：基于双向Transformer的Mask Predictor</p>
        </div>
    </header>
    
    <div class="container">
        <section id="background">
            <h2>研究背景：自回归范式的固有挑战</h2>
            <p>当前大语言模型（LLM）普遍采用自回归（AutoRegressive Modeling, AR）范式，其核心定义为：</p>
            
            <div class="formula">
                p<sub>θ</sub>(x) = p<sub>θ</sub>(x<sup>1</sup>) ∏<sub>i=2</sub><sup>L</sup> p<sub>θ</sub>(x<sup>i</sup> | x<sup>1</sup>, …, x<sup>i-1</sup>)
            </div>
            
            <p>该范式虽在商业应用中取得成功，但存在显著局限：</p>
            
            <div class="highlight">
                <h3>1. 计算不可并行化</h3>
                <p>AR模型的序列生成特性导致计算不可并行化。生成长度为L的序列需执行L次前向传播，在长文本场景（如文档生成）计算开销呈线性增长。</p>
            </div>
            
            <div class="highlight">
                <h3>2. 单向生成机制与语言双向本质相悖</h3>
                <p>AR的单向生成机制（通常从左至右）与语言的双向本质相悖。在需要逆向推理的任务中表现受限，例如诗歌补全（给定结尾补全开头）。</p>
            </div>
            
            <div class="highlight">
                <h3>3. 核心能力源于分布匹配目标</h3>
                <p>研究表明，LLM的核心能力（可扩展性、上下文学习、指令跟随）实质源于分布匹配目标：</p>
                <div class="formula">
                    min<sub>θ</sub> KL(p<sub>data</sub>(x) || p<sub>θ</sub>(x))
                </div>
                <p>而非AR结构本身。视觉领域扩散模型（DiT）的成功佐证了非AR架构同样可实现分布逼近。</p>
            </div>
            
            <p>以上三点就是LLaDA这篇工作的motivation，即解决AR模型固有的问题，探索一条新的路。</p>
        </section>
        
        <section id="results">
            <h2>LLaDA 实验结果</h2>
            
            <div class="results-grid">
                <div class="result-card">
                    <div class="result-title">Scalability</div>
                    <p>LLaDA在10<sup>23</sup> FLOPs的计算预算内能有效扩展，在六项任务上与同数据训练的ARM基线取得了可以pk的成绩。</p>
                </div>
                
                <div class="result-card">
                    <div class="result-title">逆向推理能力</div>
                    <p>在诗歌补全任务中，LLaDA展现了强大的逆向推理能力，能够根据给定的结尾补全开头部分。</p>
                </div>
                
                <div class="result-card">
                    <div class="result-title">推理实例展示</div>
                    <p>论文中提供了具体的推理实例，展示了LLaDA在多步迭代中逐步完善输出的过程。</p>
                </div>
            </div>
            
            <p>看完效果后感觉值得一读，LLaDA在多个方面展现了与自回归模型竞争甚至超越的潜力。</p>
        </section>
        
        <section id="method">
            <h2>具体做法</h2>
            <p>LLaDA试图训练的是一个基于双向Transformer的Mask Predictor。这一predictor可以从一段含有mask的序列中预测出被mask的token。</p>
            
            <h3>训练过程</h3>
            <p>为了训练这个Mask Predictor，LLaDA主要分成两步：</p>
            
            <div class="method-steps">
                <div class="step">
                    <div class="step-number">1</div>
                    <h4>随机Mask</h4>
                    <p>随机选择一个t∈(0,1)，对一个输入序列的每个token以概率t决定要不要mask。这一步我们可以获得一个被mask了一部分的序列。</p>
                </div>
                
                <div class="step">
                    <div class="step-number">2</div>
                    <h4>预测与优化</h4>
                    <p>用Mask Predictor去预测被mask的token，并计算loss回传。</p>
                </div>
            </div>
            
            <p>这种方法与BERT非常相似，主要区别在于BERT是固定概率mask一个序列，而LLaDA是随机的。</p>
            
            <h3>预训练、微调与推理</h3>
            <p>LLaDA展示了完整的训练流程：</p>
            
            <div class="comparison">
                <div class="comparison-item">
                    <div class="comparison-title">预训练阶段</div>
                    <p>流程与上述两步相同，通过大规模数据训练Mask Predictor。</p>
                </div>
                
                <div class="comparison-item">
                    <div class="comparison-title">SFT阶段</div>
                    <p>只对response做mask，使模型更好地适应特定任务。</p>
                </div>
                
                <div class="comparison-item">
                    <div class="comparison-title">推理阶段</div>
                    <p>先指定输出长度L，将prompt和L个被"mask"的token一起输入模型。通过多步迭代，逐步完善输出序列。</p>
                </div>
            </div>
            
            <div class="limitation">
                <h3>潜在局限与未来方向</h3>
                <p>目前LLaDA更像是BERT的scaling，而非严格意义上的扩散模型。正统diffusion里加噪过程很有讲究，目前LLaDA的forward过程并不太像diffusion的过程。期待未来出现真正的diffusion LLM。</p>
            </div>
        </section>
        
        <section id="conclusion">
            <h2>总结与展望</h2>
            <div class="conclusion">
                <p>LLaDA为非自回归语言模型提供了一个有前景的方向，通过双向Transformer和mask预测机制，解决了传统AR模型的多个固有局限。</p>
                <p>虽然目前的方法与严格意义上的扩散模型还有差距，但这为探索新的LLM架构开辟了道路，期待未来在这一方向上的更多突破。</p>
            </div>
        </section>
    </div>
    
    <footer>
        <div class="container">
            <p>LLaDA研究解析 &copy; 2023 - 探索语言模型新范式</p>
        </div>
    </footer>
</body>
</html>